{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from network.mynetwork_uu import Unet\n",
    "from loss.loss import CLIPLoss\n",
    "from utils.func import get_features,vgg_normalize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "lr1 = 0.0002\n",
    "lr2 = 0.0003\n",
    "model = Unet(device)\n",
    "# model = Unet().to(device)\n",
    "cliploss = CLIPLoss(device)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "vgg = torchvision.models.vgg19(pretrained=True).features.to(device)\n",
    "for x in vgg.parameters():\n",
    "    x.requires_grad = False\n",
    "\n",
    "topil = transforms.ToPILImage()\n",
    "topic = transforms.ToTensor()\n",
    "\n",
    "dir_lambda = 500\n",
    "content_lambda = 150\n",
    "patch_lambda = 9000\n",
    "norm_lambda = 0.002\n",
    "gol_lambda = 300"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "loss_li = [0]*200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train(iteration1, iteration2, pic, source, target, path):\n",
    "    input = pic\n",
    "\n",
    "    # opt = optim.Adam(model.parameters(), lr=lr1)\n",
    "    # for i in range(iteration1):\n",
    "    #     opt.zero_grad()\n",
    "    #     neo_pic = model(input)\n",
    "    #     loss = mseloss(pic, neo_pic) * 1\n",
    "    #     loss.backward()\n",
    "    #     opt.step()\n",
    "    #     pil = topil(neo_pic.squeeze(0).cpu())\n",
    "    #     print(\"iter:\", i + 1, \"loss:\", loss.item())\n",
    "    #     if ((i + 1) % 50) == 0:\n",
    "    #         pil.save(f\"./pic1/{(i + 1) // 50}.jpg\")\n",
    "    #\n",
    "    #\n",
    "    # torch.save(model,'unet.pth')\n",
    "    #\n",
    "    # cliploss.fc.requires_grad = False\n",
    "    # for x in cliploss.fc.block.parameters():\n",
    "    #     x.requires_grad = False\n",
    "\n",
    "    # model = torch.load('unet.pth')\n",
    "\n",
    "    pic_f = get_features(vgg_normalize(pic),vgg)\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=lr2)\n",
    "    for i in range(iteration2):\n",
    "\n",
    "\n",
    "        opt.zero_grad()\n",
    "        neo_pic = model(input)\n",
    "\n",
    "        dir_loss = 0\n",
    "        dir_loss += cliploss.forward_dir(pic, source, neo_pic, target)\n",
    "\n",
    "        gol_loss = 0\n",
    "        gol_loss += cliploss.forward_gol(pic, source, neo_pic, target)\n",
    "\n",
    "        content_loss = 0\n",
    "        neo_pic_f = get_features(vgg_normalize(neo_pic), vgg)\n",
    "        content_loss += torch.mean((pic_f['conv4_2'] - neo_pic_f['conv4_2']) ** 2)\n",
    "        content_loss += torch.mean((pic_f['conv5_2'] - neo_pic_f['conv5_2']) ** 2)\n",
    "        # content_loss += mseloss(pic,neo_pic)\n",
    "\n",
    "        patch_loss = 0\n",
    "        # patch_loss += cliploss.forward_patch(pic, source, neo_pic, target)\n",
    "\n",
    "        norm_loss = 0\n",
    "        norm_loss += cliploss.forward_prior(pic, source, neo_pic, target)\n",
    "\n",
    "        loss = dir_loss * dir_lambda + \\\n",
    "               content_loss * content_lambda + \\\n",
    "               patch_loss * patch_lambda + \\\n",
    "               norm_loss * norm_lambda + \\\n",
    "               gol_loss * gol_lambda\n",
    "\n",
    "        patch_loss_fast,patch_loss_slow,_ = cliploss.forward_patch_sec(pic, source, neo_pic, target)\n",
    "\n",
    "        patch_loss_fast *= patch_lambda\n",
    "        patch_loss_slow *= patch_lambda\n",
    "\n",
    "        for x in model.res2.parameters():\n",
    "            x.requires_grad = False\n",
    "        patch_loss_fast.backward(retain_graph=True)\n",
    "        for x in model.res2.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "        for x in model.res.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.conv3.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.upsample3.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.deconv3.parameters():\n",
    "            x.requires_grad = False\n",
    "        patch_loss_fast.backward(retain_graph=True)\n",
    "        for x in model.res.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.conv3.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.upsample3.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.deconv3.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # opt_fast.step()\n",
    "        # opt_slow.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # patch_loss = cliploss.forward_patch(pic, source, neo_pic, target)*patch_lambda\n",
    "            # loss_li[i]+=(loss+patch_loss).item()\n",
    "            # print(\"iter:\", i + 1, \"loss:\", (loss+patch_loss).item())\n",
    "\n",
    "            loss_li[i]+=(loss+patch_loss_fast+patch_loss_slow).item()\n",
    "            print(\"iter:\", i + 1, \"fast_loss:\", patch_loss_fast.item(), \"slow_loss:\", patch_loss_slow.item())\n",
    "            print(\"iter:\", i + 1, \"loss:\", (loss+patch_loss_fast+patch_loss_slow).item())\n",
    "\n",
    "\n",
    "        # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "        # if ((i + 1) % 10) == 0:\n",
    "        #     pil.save(f\"./pic3/{(i + 1) // 10}.jpg\")\n",
    "\n",
    "    # neo_pic = model(input)\n",
    "    # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "    # # pil.save(f\"{source}-{target}.jpg\")\n",
    "    # pil.save(path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_26508\\2585999280.py:3: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  pil = transforms.Resize(size=(512, 512), interpolation=Image.BICUBIC)(pil)\n",
      "E:\\Anaconda\\envs\\sth\\lib\\site-packages\\torchvision\\transforms\\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pil = Image.open(f\"./source_pic/boat.jpg\")\n",
    "ori_size = pil.size[::-1]\n",
    "pil = transforms.Resize(size=(512, 512), interpolation=Image.BICUBIC)(pil)\n",
    "pic = topic(pil).unsqueeze(0).to(device)\n",
    "# pic = torch.ones(1, 3, 512, 512).to(device)\n",
    "pic.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# source = \"photo\"\n",
    "source = \"blue sky\"\n",
    "\n",
    "# target = \"pop art of night city\"\n",
    "# target = \"Chinese Ink and wash painting\"\n",
    "# target = \"starry night by Van Gogh\"\n",
    "# target = \"the scream by Edvard Munch\"\n",
    "# target = \"woman\"\n",
    "# target = \"cyberpunk 2077\"\n",
    "target = \"dark black night\"\n",
    "path = \"result/result4.jpg\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 loss: 10070.07421875\n",
      "iter: 2 loss: 10020.5830078125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      4\u001B[0m usetime \u001B[38;5;241m=\u001B[39m end \u001B[38;5;241m-\u001B[39m start\n",
      "Cell \u001B[1;32mIn [4], line 94\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(iteration1, iteration2, pic, source, target, path)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;66;03m# opt_fast.step()\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# opt_slow.step()\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 94\u001B[0m     patch_loss \u001B[38;5;241m=\u001B[39m \u001B[43mcliploss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_patch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneo_pic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m*\u001B[39mpatch_lambda\n\u001B[0;32m     95\u001B[0m     loss_li[i]\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m(loss\u001B[38;5;241m+\u001B[39mpatch_loss)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miter:\u001B[39m\u001B[38;5;124m\"\u001B[39m, i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, (loss\u001B[38;5;241m+\u001B[39mpatch_loss)\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32mE:\\nada_clean\\loss\\loss.py:326\u001B[0m, in \u001B[0;36mCLIPLoss.forward_patch\u001B[1;34m(self, src_img, source_class, target_img, target_class)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward_patch\u001B[39m(\u001B[38;5;28mself\u001B[39m, src_img: torch\u001B[38;5;241m.\u001B[39mTensor, source_class: \u001B[38;5;28mstr\u001B[39m, target_img: torch\u001B[38;5;241m.\u001B[39mTensor, target_class: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    324\u001B[0m \n\u001B[0;32m    325\u001B[0m     \u001B[38;5;66;03m# dir_loss = 1 * self.clip_directional_loss(src_img, source_class, target_img, target_class)\u001B[39;00m\n\u001B[1;32m--> 326\u001B[0m     patch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_directional_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_class\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;66;03m# dir_loss = 1 * self.global_clip_loss(target_img, f\"a {target_class}\")\u001B[39;00m\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;66;03m# dir_loss += 1 * self.clip_angle_loss(src_img, source_class, target_img, target_class)\u001B[39;00m\n\u001B[0;32m    329\u001B[0m     \u001B[38;5;66;03m# print(loss1.item(),loss2.item(),loss3.item(),loss4.item())\u001B[39;00m\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;66;03m# loss += loss1 + loss2 + loss3 + loss4\u001B[39;00m\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;66;03m# loss.requires_grad = True\u001B[39;00m\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m patch_loss\n",
      "File \u001B[1;32mE:\\nada_clean\\loss\\loss.py:276\u001B[0m, in \u001B[0;36mCLIPLoss.patch_directional_loss\u001B[1;34m(self, src_img, source_class, target_img, target_class)\u001B[0m\n\u001B[0;32m    273\u001B[0m src_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_image_features(src_patches)\n\u001B[0;32m    275\u001B[0m target_patches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_patches(target_img, patch_points, patch_size)\n\u001B[1;32m--> 276\u001B[0m target_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_image_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_patches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;66;03m# src_feature = self.get_image_features(src_img)\u001B[39;00m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;66;03m# src_features = src_feature.expand(target_features.shape)\u001B[39;00m\n\u001B[0;32m    281\u001B[0m edit_direction \u001B[38;5;241m=\u001B[39m (target_features \u001B[38;5;241m-\u001B[39m src_features)\n",
      "File \u001B[1;32mE:\\nada_clean\\loss\\loss.py:181\u001B[0m, in \u001B[0;36mCLIPLoss.get_image_features\u001B[1;34m(self, img, norm)\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_image_features\u001B[39m(\u001B[38;5;28mself\u001B[39m, img: torch\u001B[38;5;241m.\u001B[39mTensor, norm: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 181\u001B[0m     image_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m norm:\n\u001B[0;32m    184\u001B[0m         image_features \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m image_features\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mnorm(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mE:\\nada_clean\\loss\\loss.py:159\u001B[0m, in \u001B[0;36mCLIPLoss.encode_images\u001B[1;34m(self, images)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode_images\u001B[39m(\u001B[38;5;28mself\u001B[39m, images: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# images = torch.nn.functional.interpolate(images, (224,224), mode=\"bilinear\")\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;66;03m# images = self.preprocess(images.squeeze(0).cpu())\u001B[39;00m\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;66;03m# images = images.to(self.device).unsqueeze(0)\u001B[39;00m\n\u001B[1;32m--> 159\u001B[0m     images \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m     code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mencode_image(images)\n\u001B[0;32m    161\u001B[0m     code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(code)\n",
      "File \u001B[1;32mE:\\Anaconda\\envs\\sth\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[0;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[0;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[0;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[1;32mE:\\nada_clean\\loss\\loss.py:79\u001B[0m, in \u001B[0;36mPreprocess.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     77\u001B[0m mean \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m0.48145466\u001B[39m, \u001B[38;5;241m0.4578275\u001B[39m, \u001B[38;5;241m0.40821073\u001B[39m])\n\u001B[0;32m     78\u001B[0m mean \u001B[38;5;241m=\u001B[39m mean\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 79\u001B[0m mean \u001B[38;5;241m=\u001B[39m \u001B[43mmean\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m std \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m0.26862954\u001B[39m, \u001B[38;5;241m0.26130258\u001B[39m, \u001B[38;5;241m0.27577711\u001B[39m])\n\u001B[0;32m     81\u001B[0m std \u001B[38;5;241m=\u001B[39m std\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train(200, 200, pic, source, target, path)\n",
    "end = time.time()\n",
    "usetime = end - start\n",
    "print(f\"usetime: {usetime}\")\n",
    "\n",
    "# 198.63224363327026"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neo_pic = model(pic)\n",
    "pil = topil(neo_pic.squeeze(0).cpu())\n",
    "pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n",
    "pil.save(path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_cmp_inner = open(\"neo_cmp_inner.txt\", \"r\")\n",
    "read_loss_li = file_cmp_inner.readline()\n",
    "read_loss_li = [float(x) for x in read_loss_li.split()]\n",
    "if not read_loss_li:\n",
    "    read_loss_li = [0]*200\n",
    "\n",
    "cur_times = int(file_cmp_inner.readline())\n",
    "\n",
    "file_cmp_inner.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i for i in range(200)]\n",
    "loss_li = [x/(cur_times+1) for x in loss_li]\n",
    "plt.plot(x,loss_li,color=\"red\",marker=\"o\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    loss_li[i]+=read_loss_li[i]\n",
    "\n",
    "with open(file = \"neo_cmp_inner.txt\", mode = \"w\") as file:\n",
    "    for i in loss_li:\n",
    "        file.write(str(i)+\" \")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(str(cur_times+1))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}